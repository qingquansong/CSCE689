{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from model import generate_model\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qq/CSCE689/3D-ResNets-PyTorch'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# opt = parse_opts()\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#         '--root_path',\n",
    "#         default='/root/data/ActivityNet',\n",
    "#         type=str,\n",
    "#         help='Root directory path of data')\n",
    "# parser.add_argument(\n",
    "#         '--video_path',\n",
    "#         default='video_kinetics_jpg',\n",
    "#         type=str,\n",
    "#         help='Directory path of Videos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = easydict.EasyDict({\n",
    "    \"root_path\": '',\n",
    "    \"video_path\": 'video_kinetics_jpg',\n",
    "    \"annotation_path\": 'kinetics.json',\n",
    "    \"result_path\": 'results',\n",
    "    \"dataset\": 'ucf101',\n",
    "    \"n_classes\": 101,\n",
    "    \"n_finetune_classes\": 101,\n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 16,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'activitynet',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_epochs\": 200,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"resume_path\": '',\n",
    "    \"pretrain_path\": '',\n",
    "    \"ft_begin_index\": 0,\n",
    "    \"no_train\": False,\n",
    "    \"no_val\": False,\n",
    "    \"test\": False,\n",
    "    \"test_subset\": 'val',\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 10,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"model_depth\": 18,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_val': False, 'learning_rate': 0.1, 'n_val_samples': 3, 'scale_step': 0.84089641525, 'std': [38.7568578, 37.88248729, 40.02898126], 'sample_size': 112, 'crop_position_in_test': 'c', 'mean': [114.7748, 107.7354, 99.475], 'result_path': 'results', 'lr_patience': 10, 'pretrain_path': '', 'scale_in_test': 1.0, 'begin_epoch': 1, 'momentum': 0.9, 'n_scales': 5, 'checkpoint': 10, 'train_crop': 'corner', 'scales': [1.0, 0.84089641525, 0.7071067811803005, 0.5946035574934808, 0.4999999999911653], 'wide_resnet_k': 2, 'weight_decay': 0.001, 'no_cuda': False, 'batch_size': 128, 'resnet_shortcut': 'B', 'resnext_cardinality': 32, 'model_depth': 18, 'optimizer': 'sgd', 'arch': 'resnet-18', 'annotation_path': 'kinetics.json', 'no_softmax_in_test': False, 'initial_scale': 1.0, 'nesterov': False, 'no_train': False, 'ft_begin_index': 0, 'n_finetune_classes': 400, 'manual_seed': 1, 'sample_duration': 16, 'dampening': 0.9, 'no_hflip': False, 'mean_dataset': 'activitynet', 'n_epochs': 200, 'dataset': 'kinetics', 'test': False, 'n_classes': 400, 'no_mean_norm': False, 'model': 'resnet', 'video_path': 'video_kinetics_jpg', 'std_norm': False, 'test_subset': 'val', 'n_threads': 4, 'norm_value': 1, 'root_path': '', 'resume_path': ''}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if opt.root_path != '':\n",
    "    opt.video_path = os.path.join(opt.root_path, opt.video_path)\n",
    "    opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n",
    "    opt.result_path = os.path.join(opt.root_path, opt.result_path)\n",
    "    if opt.resume_path:\n",
    "        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n",
    "    if opt.pretrain_path:\n",
    "        opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n",
    "opt.scales = [opt.initial_scale]\n",
    "for i in range(1, opt.n_scales):\n",
    "    opt.scales.append(opt.scales[-1] * opt.scale_step)\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\n",
    "opt.std = get_std(opt.norm_value)\n",
    "print(opt)\n",
    "with open(os.path.join(opt.result_path, 'opts.json'), 'w') as opt_file:\n",
    "    json.dump(vars(opt), opt_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnet.py:145: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool3d(kernel_size=(1, 4, 4), stride=1, padding=0)\n",
      "    (fc): Linear(in_features=512, out_features=400, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(opt.manual_seed)\n",
    "\n",
    "model, parameters = generate_model(opt)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if not opt.no_cuda:\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "if opt.no_mean_norm and not opt.std_norm:\n",
    "    norm_method = Normalize([0, 0, 0], [1, 1, 1])\n",
    "elif not opt.std_norm:\n",
    "    norm_method = Normalize(opt.mean, [1, 1, 1])\n",
    "else:\n",
    "    norm_method = Normalize(opt.mean, opt.std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kinetics.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-11a89913c0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtarget_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     training_data = get_training_set(opt, spatial_transform,\n\u001b[0;32m---> 18\u001b[0;31m                                      temporal_transform, target_transform)\n\u001b[0m\u001b[1;32m     19\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[1;32m     20\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSCE689/3D-ResNets-PyTorch/dataset.py\u001b[0m in \u001b[0;36mget_training_set\u001b[0;34m(opt, spatial_transform, temporal_transform, target_transform)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mspatial_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtemporal_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemporal_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             target_transform=target_transform)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'activitynet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         training_data = ActivityNet(\n",
      "\u001b[0;32m~/CSCE689/3D-ResNets-PyTorch/datasets/kinetics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, annotation_path, subset, n_samples_for_each_video, spatial_transform, temporal_transform, target_transform, sample_duration, get_loader)\u001b[0m\n\u001b[1;32m    167\u001b[0m         self.data, self.class_names = make_dataset(\n\u001b[1;32m    168\u001b[0m             \u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples_for_each_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             sample_duration)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSCE689/3D-ResNets-PyTorch/datasets/kinetics.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(root_path, annotation_path, subset, n_samples_for_each_video, sample_duration)\u001b[0m\n\u001b[1;32m     85\u001b[0m def make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n\u001b[1;32m     86\u001b[0m                  sample_duration):\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_annotation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mvideo_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_names_and_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSCE689/3D-ResNets-PyTorch/datasets/kinetics.py\u001b[0m in \u001b[0;36mload_annotation_data\u001b[0;34m(data_file_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_annotation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kinetics.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "if not opt.no_train:\n",
    "    assert opt.train_crop in ['random', 'corner', 'center']\n",
    "    if opt.train_crop == 'random':\n",
    "        crop_method = MultiScaleRandomCrop(opt.scales, opt.sample_size)\n",
    "    elif opt.train_crop == 'corner':\n",
    "        crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size)\n",
    "    elif opt.train_crop == 'center':\n",
    "        crop_method = MultiScaleCornerCrop(\n",
    "            opt.scales, opt.sample_size, crop_positions=['c'])\n",
    "    spatial_transform = Compose([\n",
    "        crop_method,\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(opt.norm_value), norm_method\n",
    "    ])\n",
    "    temporal_transform = TemporalRandomCrop(opt.sample_duration)\n",
    "    target_transform = ClassLabel()\n",
    "    training_data = get_training_set(opt, spatial_transform,\n",
    "                                     temporal_transform, target_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        training_data,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=opt.n_threads,\n",
    "        pin_memory=True)\n",
    "    train_logger = Logger(\n",
    "        os.path.join(opt.result_path, 'train.log'),\n",
    "        ['epoch', 'loss', 'acc', 'lr'])\n",
    "    train_batch_logger = Logger(\n",
    "        os.path.join(opt.result_path, 'train_batch.log'),\n",
    "        ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])\n",
    "\n",
    "    if opt.nesterov:\n",
    "        dampening = 0\n",
    "    else:\n",
    "        dampening = opt.dampening\n",
    "    optimizer = optim.SGD(\n",
    "        parameters,\n",
    "        lr=opt.learning_rate,\n",
    "        momentum=opt.momentum,\n",
    "        dampening=dampening,\n",
    "        weight_decay=opt.weight_decay,\n",
    "        nesterov=opt.nesterov)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=opt.lr_patience)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not opt.no_val:\n",
    "    spatial_transform = Compose([\n",
    "        Scale(opt.sample_size),\n",
    "        CenterCrop(opt.sample_size),\n",
    "        ToTensor(opt.norm_value), norm_method\n",
    "    ])\n",
    "    temporal_transform = LoopPadding(opt.sample_duration)\n",
    "    target_transform = ClassLabel()\n",
    "    validation_data = get_validation_set(\n",
    "        opt, spatial_transform, temporal_transform, target_transform)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_data,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.n_threads,\n",
    "        pin_memory=True)\n",
    "    val_logger = Logger(\n",
    "        os.path.join(opt.result_path, 'val.log'), ['epoch', 'loss', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.resume_path:\n",
    "    print('loading checkpoint {}'.format(opt.resume_path))\n",
    "    checkpoint = torch.load(opt.resume_path)\n",
    "    assert opt.arch == checkpoint['arch']\n",
    "\n",
    "    opt.begin_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if not opt.no_train:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('run')\n",
    "for i in range(opt.begin_epoch, opt.n_epochs + 1):\n",
    "    if not opt.no_train:\n",
    "        train_epoch(i, train_loader, model, criterion, optimizer, opt,\n",
    "                    train_logger, train_batch_logger)\n",
    "    if not opt.no_val:\n",
    "        validation_loss = val_epoch(i, val_loader, model, criterion, opt,\n",
    "                                    val_logger)\n",
    "\n",
    "    if not opt.no_train and not opt.no_val:\n",
    "        scheduler.step(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.test:\n",
    "    spatial_transform = Compose([\n",
    "        Scale(int(opt.sample_size / opt.scale_in_test)),\n",
    "        CornerCrop(opt.sample_size, opt.crop_position_in_test),\n",
    "        ToTensor(opt.norm_value), norm_method\n",
    "    ])\n",
    "    temporal_transform = LoopPadding(opt.sample_duration)\n",
    "    target_transform = VideoID()\n",
    "\n",
    "    test_data = get_test_set(opt, spatial_transform, temporal_transform,\n",
    "                             target_transform)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.n_threads,\n",
    "        pin_memory=True)\n",
    "    test.test(test_loader, model, opt, test_data.class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
