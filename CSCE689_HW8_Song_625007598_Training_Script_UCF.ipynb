{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of the improvements in HW8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last homework, we do two things to improve the model:\n",
    "\n",
    "1. Manually correct some mislabeled information in the previous datasets. For example, the `Drumming` class and `Band Marching` class are errorly labeled as \"not instrumental playing action\", however, these are not the cases. Although the generall accuracy is not improved, but since previous testing data also contained the mislabed information, the current testing labels are also corrected. Thus, the model performance now becomes more stable and more convincing.\n",
    "\n",
    "2. Fine-tune hyperparameters of the ensemble model (regularization hyperparameter C in the logistic regression clf).\n",
    "\n",
    "3. Try different ensemble strategy (does not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'hw8_results',      # 'Result directory path'\n",
    "    \"n_classes\": 2,                 # 'Number of classes for fine-tuning'\n",
    "    \"sample_size\": 112,             # 'Height and width of inputs'\n",
    "    \"sample_duration\": 16,          # 'Temporal duration of inputs'\n",
    "    \"initial_scale\": 1.0,           # 'Initial scale for multiscale cropping')\n",
    "    \"n_scales\": 5,                  # 'Number of scales for multiscale cropping'\n",
    "    \"scale_step\": 0.84089641525,    # 'Scale step for multiscale cropping'\n",
    "    \"train_crop\": 'corner',         # 'Spatial cropping method in training. Corner is selection from 4 corners and 1 center.\n",
    "    \"learning_rate\": 0.1,           # 'Initial learning rate (divided by 10 while training by lr scheduler)'\n",
    "    \"momentum\": 0.9,                # 'Momentum'\n",
    "    \"dampening\": 0.9,               # 'Dampening of SGD'\n",
    "    \"weight_decay\": 0.001,          # 'Weight decay in SGD'\n",
    "    \"mean_dataset\": 'kinetics',     # 'Dataset for mean values of mean subtraction'\n",
    "    \"no_mean_norm\": False,          # 'If true, inputs are not normalized by mean'\n",
    "    \"std_norm\": False,              # 'If true, inputs are normalized by standard deviation'\n",
    "    \"nesterov\": False,              # 'Nesterov momentum'\n",
    "    \"optimizer\": 'sgd',             # 'Optimizer type'\n",
    "    \"lr_patience\": 10,              # 'Patience of LR scheduler. See documentation of Reduce on Loss Plateau Decay (ReduceLROnPlateau). https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay'\n",
    "    \"batch_size\": 256,               # 'Batch size'\n",
    "    \"n_epochs\": 2,                  # 'Fine-tuning epochs'\n",
    "    \"begin_epoch\": 1,               # 'Training begins at this epoch. Previous trained model indicated by resume_path is loaded.'\n",
    "    \"n_val_samples\": 3,             # 'Number of validation samples for each activity'\n",
    "    \"ft_begin_index\": 5,            # 'Begin block index of fine-tuning'\n",
    "    \"scale_in_test\": 1.0,           # 'Spatial scale in test'\n",
    "    \"crop_position_in_test\": 'c',   #  Cropping method: center cropping\n",
    "    \"no_softmax_in_test\": False,    # 'If true, output for each clip is not normalized using softmax.'\n",
    "    \"no_cuda\": False,               # 'If true, cuda is not used.'\n",
    "    \"n_threads\": 3,                 # 'Number of threads for multi-thread loading'\n",
    "    \"checkpoint\": 2,                #  Trained model is saved at every this epochs'\n",
    "    \"no_hflip\": False,              # 'If true holizontal flipping is not performed'\n",
    "    \"norm_value\": 1,                # 'If 1, range of inputs is [0-255]. If 255, range of inputs is [0-1].'\n",
    "    \"model_depth\": 101,             # 'Depth of resnet \n",
    "    \"resnet_shortcut\": 'B',         # 'Shortcut type of resnet' to align the dimensionality for skip connection\n",
    "    \"wide_resnet_k\": 2,             # 'Wide resnet k'\n",
    "    \"resnext_cardinality\": 32,      # 'ResNeXt cardinality'\n",
    "    \"manual_seed\": 1,               # 'Manually set random seed'\n",
    "    'test_subset': 'test',          # 'Used test subset name'\n",
    "})\n",
    "# opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path = '/data/qq/CSCE689/'\n",
    "opt.video_path = opt.root_path + 'video/UCF-101-jpg/'\n",
    "opt.annotation_path = opt.root_path + 'video/UCF-music-annotation/ucf_binary_music_annotation_correct.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess UCF-music dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/9537]\n",
      "dataset loading [1000/9537]\n",
      "dataset loading [2000/9537]\n",
      "dataset loading [3000/9537]\n",
      "dataset loading [4000/9537]\n",
      "dataset loading [5000/9537]\n",
      "dataset loading [6000/9537]\n",
      "dataset loading [7000/9537]\n",
      "dataset loading [8000/9537]\n",
      "dataset loading [9000/9537]\n",
      "dataset loading [0/9537]\n",
      "dataset loading [1000/9537]\n",
      "dataset loading [2000/9537]\n",
      "dataset loading [3000/9537]\n",
      "dataset loading [4000/9537]\n",
      "dataset loading [5000/9537]\n",
      "dataset loading [6000/9537]\n",
      "dataset loading [7000/9537]\n",
      "dataset loading [8000/9537]\n",
      "dataset loading [9000/9537]\n",
      "dataset loading [0/2183]\n",
      "dataset loading [1000/2183]\n",
      "dataset loading [2000/2183]\n",
      "dataset loading [0/2183]\n",
      "dataset loading [1000/2183]\n",
      "dataset loading [2000/2183]\n",
      "dataset loading [0/1600]\n",
      "dataset loading [1000/1600]\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed video frames and annotation\n",
    "from datasets.ucf101_binary import UCF101\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = ClassLabel() # VideoID() # \n",
    "\n",
    "# get training data\n",
    "training_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'training',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "# wrap training data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False) # True\n",
    "\n",
    "\n",
    "# get training data\n",
    "training_data2 = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'training',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=VideoID(),\n",
    "    sample_duration=16)\n",
    "\n",
    "# wrap training data\n",
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    training_data2,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=5,\n",
    "    pin_memory=False) # True\n",
    "\n",
    "\n",
    "\n",
    "# get validation data\n",
    "val_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'validation',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "# wrap validation data\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False) \n",
    "\n",
    "# get validation data\n",
    "val_data2 = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'validation',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=VideoID(),\n",
    "    sample_duration=16)\n",
    "\n",
    "# wrap validation data\n",
    "val_loader2 = torch.utils.data.DataLoader(\n",
    "    val_data2,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=5,\n",
    "    pin_memory=False) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Directly predict videoID for test data\n",
    "target_transform = VideoID()\n",
    "\n",
    "# get test data\n",
    "test_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features of ResNeXt - 101 and save in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import resnext\n",
    "\n",
    "# # construct model architecture\n",
    "# model_rxt101 = resnext.resnet101(\n",
    "#                 num_classes=opt.n_classes,\n",
    "#                 shortcut_type=opt.resnet_shortcut,\n",
    "#                 cardinality=opt.resnext_cardinality,\n",
    "#                 sample_size=opt.sample_size,\n",
    "#                 sample_duration=opt.sample_duration)\n",
    "\n",
    "# model_rxt101 = model_rxt101.cuda()\n",
    "# # wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "# model_rxt101 = nn.DataParallel(model_rxt101, device_ids=None)\n",
    "\n",
    "\n",
    "# ### Load pretrained weight\n",
    "# # customize the pretrained model path\n",
    "# pretrain = torch.load(opt.root_path + 'pretrain/resnext-101-kinetics.pth')\n",
    "# pretrain_dict = pretrain['state_dict']\n",
    "\n",
    "# # do not load the last layer since we want to fine-tune it\n",
    "# pretrain_dict.pop('module.fc.weight')\n",
    "# pretrain_dict.pop('module.fc.bias')\n",
    "# model_dict = model_rxt101.state_dict()\n",
    "# model_dict.update(pretrain_dict) \n",
    "# model_rxt101.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # register layer index to extract the features by forwarding all the video clips\n",
    "# activation = {}\n",
    "# def get_activation(name):\n",
    "#     def hook(model, input, output):\n",
    "#         activation[name] = output.detach()\n",
    "#     return hook\n",
    "\n",
    "# model_rxt101.module.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "# model_rxt101.eval()\n",
    "\n",
    "\n",
    "# # forward all the videos to extract features\n",
    "# avgpool, avgpool_val, avgpool_test = [], [], []\n",
    "# targets, targets_val, targets_test = [], [], []\n",
    "# with torch.no_grad():\n",
    "#     print(\"Extract train set features:\")\n",
    "#     for i, (inputs, target) in enumerate(train_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rxt101(inputs)\n",
    "#         avgpool.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "#         targets.append(target)\n",
    "    \n",
    "#     print(\"Extract val set features:\")\n",
    "#     for i, (inputs, target) in enumerate(val_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rxt101(inputs)\n",
    "#         avgpool_val.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "#         targets_val.append(target)\n",
    " \n",
    "#     print(\"Extract test set features:\")\n",
    "#     for i, (inputs, target) in enumerate(test_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rxt101(inputs)\n",
    "#         avgpool_test.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "#         targets_test.append(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the features\n",
    "# avgpool_np = np.concatenate([i.numpy() for i in avgpool], axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/resnext101_avgpool_train.npy', avgpool_np)\n",
    "\n",
    "# targets_np = np.concatenate([i.numpy() for i in targets], axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/class_names_ucf_train_correct.npy', targets_np)\n",
    "\n",
    "# avgpool_val_np = np.concatenate([i.numpy() for i in avgpool_val], axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/resnext101_avgpool_val.npy', avgpool_val_np)\n",
    "\n",
    "# targets_val_np = np.concatenate([i.numpy() for i in targets_val], axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/class_names_ucf_val_correct.npy', targets_val_np)\n",
    "\n",
    "# avgpool_test_np = np.concatenate([i.numpy() for i in avgpool_test], axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/resnext101_avgpool_test.npy', avgpool_test_np)\n",
    "\n",
    "# targets_test_np = np.concatenate(np.array(targets_test), axis=0)\n",
    "# np.save(opt.root_path + 'feature_ucf_all/class_names_ucf_test_correct.npy', targets_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features of ResNet-50 and save in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import resnet\n",
    "\n",
    "# # construct model architecture\n",
    "# model_rt50 = resnet.resnet50(\n",
    "#                 num_classes=opt.n_classes,\n",
    "#                 shortcut_type=opt.resnet_shortcut,\n",
    "#                 sample_size=opt.sample_size,\n",
    "#                 sample_duration=opt.sample_duration)\n",
    "\n",
    "# model_rt50 = model_rt50.cuda()\n",
    "# # wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "# model_rt50 = nn.DataParallel(model_rt50, device_ids=None)\n",
    "\n",
    "\n",
    "# ### Load pretrained weight\n",
    "# # customize the pretrained model path\n",
    "# pretrain = torch.load(opt.root_path + 'pretrain/resnet-50-kinetics.pth')\n",
    "# pretrain_dict = pretrain['state_dict']\n",
    "\n",
    "# # do not load the last layer since we want to fine-tune it\n",
    "# pretrain_dict.pop('module.fc.weight')\n",
    "# pretrain_dict.pop('module.fc.bias')\n",
    "# model_dict = model_rt50.state_dict()\n",
    "# model_dict.update(pretrain_dict) \n",
    "# model_rt50.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # register layer index to extract the features by forwarding all the video clips\n",
    "# activation = {}\n",
    "# def get_activation(name):\n",
    "#     def hook(model, input, output):\n",
    "#         activation[name] = output.detach()\n",
    "#     return hook\n",
    "\n",
    "# model_rt50.module.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "# model_rt50.eval()\n",
    "\n",
    "\n",
    "# # forward all the videos to extract features\n",
    "# avgpool, avgpool_val, avgpool_test = [], [], []\n",
    "# with torch.no_grad():\n",
    "#     print(\"Extract val set features:\")\n",
    "#     for i, (inputs, target) in enumerate(val_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rt50(inputs)\n",
    "#         avgpool_val.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    " \n",
    "#     # save the features\n",
    "#     avgpool_val_np = np.concatenate([i.numpy() for i in avgpool_val], axis=0)\n",
    "#     np.save(opt.root_path + 'feature_ucf_all/resnet50_avgpool_val.npy', avgpool_val_np)\n",
    "    \n",
    "    \n",
    "#     print(\"Extract test set features:\")\n",
    "#     for i, (inputs, target) in enumerate(test_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rt50(inputs)\n",
    "#         avgpool_test.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "        \n",
    "#     # save the features\n",
    "#     avgpool_test_np = np.concatenate([i.numpy() for i in avgpool_test], axis=0)\n",
    "#     np.save(opt.root_path + 'feature_ucf_all/resnet50_avgpool_test.npy', avgpool_test_np)    \n",
    " \n",
    "#     print(\"Extract train set features:\")\n",
    "#     for i, (inputs, target) in enumerate(train_loader):\n",
    "#         if i % 30 == 0:\n",
    "#             print(i)\n",
    "#         output = model_rt50(inputs)\n",
    "#         avgpool.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "    \n",
    "#     # save the features\n",
    "#     avgpool_np = np.concatenate([i.numpy() for i in avgpool], axis=0)\n",
    "#     np.save(opt.root_path + 'feature_ucf_all/resnet50_avgpool_train.npy', avgpool_np)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and fuse (concatenate) features and fine-tune an appended classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_train.npy')\n",
    "x_train_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_train.npy')\n",
    "\n",
    "x_val_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_val.npy')\n",
    "x_val_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_val.npy')\n",
    "\n",
    "x_test_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_test.npy')\n",
    "x_test_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_test.npy')\n",
    "\n",
    "y_train = 1 - np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_train_correct.npy')\n",
    "y_val = 1 - np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_val_correct.npy')\n",
    "y_test = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test_correct.npy')\n",
    "y = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "x_train = np.concatenate([x_train_1, x_train_2], axis=1)\n",
    "x_val = np.concatenate([x_val_1, x_val_2], axis=1)\n",
    "x_test = np.concatenate([x_test_1, x_test_2], axis=1)\n",
    "X = np.concatenate([x_train, x_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114779, 4096),\n",
       " (26018, 4096),\n",
       " (18873, 4096),\n",
       " (114779,),\n",
       " (26018,),\n",
       " array(['v_BasketballDunk_g01_c02', 'v_BasketballDunk_g01_c02',\n",
       "        'v_BasketballDunk_g01_c02', ..., 'v_Nunchucks_g06_c03',\n",
       "        'v_Nunchucks_g06_c03', 'v_Nunchucks_g06_c03'], dtype='<U28'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18873, 1587, 18873)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(opt.annotation_path) as rfp:\n",
    "    annotations = json.load(rfp)\n",
    "\n",
    "ground_truth_all = []\n",
    "for k in y_test:\n",
    "    ground_truth_all.append(annotations['database'][k]['annotations']['label'])\n",
    "    \n",
    "    \n",
    "name_to_int = {v:1-k for k,v in test_data.class_names.items()}\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "ground_truth = []\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        ground_truth.append(name_to_int[ground_truth_all[split]])\n",
    "        \n",
    "ground_truth_all_num = []\n",
    "for i in ground_truth_all:\n",
    "    ground_truth_all_num.append(name_to_int[i])\n",
    "    \n",
    "len(ground_truth_all), len(ground_truth), len(ground_truth_all_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the pretrained model on the targe UCF-101 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit three logistic regression classifier on (1) ResNeXt features (2) ResNet features (3) Combined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # Create logistic Regression Classifiers\n",
    "# clf1 = LogisticRegression(random_state=0, C=1)\n",
    "# clf2 = LogisticRegression(random_state=0, C=1)\n",
    "# clf = LogisticRegression(random_state=0, C=0.5)\n",
    "\n",
    "# # Train the model using the training sets\n",
    "# clf1.fit(x_train_1, y_train)\n",
    "# clf2.fit(x_train_2, y_train)    \n",
    "# clf.fit(x_train, y_train) # combine  \n",
    "                         \n",
    "# # save model\n",
    "# dump(clf1, './hw8_results/logistic1_ucf.joblib') \n",
    "# dump(clf2, './hw8_results/logistic2_ucf.joblib') \n",
    "# dump(clf, './hw8_results/logistic_ucf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "clf1 = load('./hw8_results/logistic1_ucf.joblib') \n",
    "clf2 = load('./hw8_results/logistic2_ucf.joblib') \n",
    "clf = load('./hw8_results/logistic_ucf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Test Probability\n",
    "y_test_prob1 = clf1.predict_proba(x_test_1)\n",
    "y_test_prob2 = clf2.predict_proba(x_test_2)\n",
    "y_test_prob = clf.predict_proba(x_test)\n",
    "\n",
    "# y_test_prob1 = clf1.predict(x_test_1)\n",
    "# y_test_prob2 = clf2.predict(x_test_2)\n",
    "# y_test_prob = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test, compare the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNeXt-101 feature +  Resnet-50 feature + logisticRegressionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Accuracy: 0.9918084436042848\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Yes       1.00      0.99      1.00      1408\n",
      "          No       0.94      0.99      0.96       179\n",
      "\n",
      "    accuracy                           0.99      1587\n",
      "   macro avg       0.97      0.99      0.98      1587\n",
      "weighted avg       0.99      0.99      0.99      1587\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get ground-truth split\n",
    "name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test = []\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test.append(np.argmax(np.mean(y_test_prob[split:split_idx[i+1]], axis=0)))\n",
    "        \n",
    "# Display the testing results\n",
    "acc = accuracy_score(ground_truth, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"General Accuracy: {}\\n\".format(accuracy_score(ground_truth, y_pred_test)))\n",
    "print(classification_report(ground_truth, y_pred_test, target_names=list(test_data.class_names.values())))\n",
    "\n",
    "cm = confusion_matrix(ground_truth, y_pred_test, labels=list(test_data.class_names.keys()))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "fig=plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(test_data.class_names.values()), rotation=30); \n",
    "ax.yaxis.set_ticklabels(list(test_data.class_names.values()), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNeXt-101 feature + logisticRegressionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground-truth split\n",
    "name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test1 = []\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test1.append(np.argmax(np.mean(y_test_prob1[split:split_idx[i+1]], axis=0)))\n",
    "        \n",
    "# Display the testing results\n",
    "acc = accuracy_score(ground_truth, y_pred_test1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"General Accuracy: {}\\n\".format(accuracy_score(ground_truth, y_pred_test1)))\n",
    "print(classification_report(ground_truth, y_pred_test1, target_names=list(test_data.class_names.values())))\n",
    "\n",
    "cm = confusion_matrix(ground_truth, y_pred_test1, labels=list(test_data.class_names.keys()))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "fig=plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(test_data.class_names.values()), rotation=30); \n",
    "ax.yaxis.set_ticklabels(list(test_data.class_names.values()), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resnet-50 feature + logisticRegressionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground-truth split\n",
    "name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test2 = []\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test2.append(np.argmax(np.mean(y_test_prob2[split:split_idx[i+1]], axis=0)))\n",
    "        \n",
    "# Display the testing results\n",
    "acc = accuracy_score(ground_truth, y_pred_test2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"General Accuracy: {}\\n\".format(accuracy_score(ground_truth, y_pred_test2)))\n",
    "print(classification_report(ground_truth, y_pred_test2, target_names=list(test_data.class_names.values())))\n",
    "\n",
    "cm = confusion_matrix(ground_truth, y_pred_test2, labels=list(test_data.class_names.keys()))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "fig=plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(test_data.class_names.values()), rotation=30); \n",
    "ax.yaxis.set_ticklabels(list(test_data.class_names.values()), rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
