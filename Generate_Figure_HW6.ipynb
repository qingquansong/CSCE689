{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'results2',\n",
    "    \"dataset\": 'ucf101-music', # 'ucf101',\n",
    "    \"n_classes\": 9, \n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 16,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'activitynet',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 2,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"ft_begin_index\": 5,\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 2,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"pretained_model_name\": 'resnext-101-kinetics',\n",
    "    \"model_depth\": 101,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1,\n",
    "    'test_subset': 'test',\n",
    "})\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path = '/data/qq/CSCE689/'\n",
    "opt.video_path = opt.root_path + 'video/UCF-101-jpg/'\n",
    "opt.annotation_path = opt.root_path + 'video/UCF-music-annotation/ucf_binary_music_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnext\n",
    "\n",
    "# construct model architecture\n",
    "model = resnext.resnet101(\n",
    "            num_classes=opt.n_classes,\n",
    "            shortcut_type=opt.resnet_shortcut,\n",
    "            cardinality=opt.resnext_cardinality,\n",
    "            sample_size=opt.sample_size,\n",
    "            sample_duration=opt.sample_duration)\n",
    "\n",
    "model = model.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "# load best weight (we can also refit the model on the combined train-val dataset, \n",
    "# but here we simple load the weight and do the final testing)\n",
    "pretrain = torch.load('./results1/save_50.pth')\n",
    "model.load_state_dict(pretrain['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/1600]\n",
      "dataset loading [1000/1600]\n"
     ]
    }
   ],
   "source": [
    "from datasets.ucf101 import UCF101\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = VideoID() # ClassLabel()\n",
    "\n",
    "\n",
    "\n",
    "# get test data\n",
    "test_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['v_BasketballDunk_g01_c02', \n",
    "            'v_Drumming_g05_c06', \n",
    "            'v_Rafting_g07_c03', \n",
    "            'v_PlayingFlute_g07_c04',\n",
    "            'v_PlayingPiano_g01_c02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvns = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "avi_path = \"/data/qq/CSCE689/video/UCF-101\"\n",
    "clip_duration_dict = {}\n",
    "real_prediction_dict = {}\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "for tvn in examples:\n",
    "    print()\n",
    "    clip = VideoFileClip(os.path.join(avi_path, tvn[2:-8], tvn + \".avi\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration]\n",
    "#     real_prediction_dict[tvn] = test_results['results'][tvn][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v_BasketballDunk_g01_c02': [2.6],\n",
       " 'v_Drumming_g05_c06': [8.44],\n",
       " 'v_Rafting_g07_c03': [7.28],\n",
       " 'v_PlayingFlute_g07_c04': [16.28],\n",
       " 'v_PlayingPiano_g01_c02': [7.37]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_duration_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./hw6_results/clip_duration_dict_example.json', 'w') as fp:\n",
    "#     json.dump(clip_duration_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "clf = load('./hw6_results/logistic_ucf.joblib') \n",
    "\n",
    "x_test_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_train.npy')\n",
    "x_test_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_train.npy')\n",
    "x_test = np.concatenate([x_test_1, x_test_2], axis=1)\n",
    "y_test = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "y_pred_test_raw = clf.predict(x_test)\n",
    "y_pred_test_prob_raw = clf.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# get ground-truth split\n",
    "# name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "name_to_int = {'Yes': 1, 'No': 0}\n",
    "\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test, y_pred_test_prob, y_pred_test_final = {}, {}, {}\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test[y_test[split]] = y_pred_test_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_prob[y_test[split]] = y_pred_test_prob_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_final[y_test[split]] = np.argmax(np.mean(y_pred_test_prob_raw[split:split_idx[i+1]], axis=0))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "music = ['PlayingCello',  'PlayingDhol',   'PlayingGuitar',  'PlayingSitar',  'PlayingViolin', \n",
    "         'PlayingDaf',    'PlayingFlute',  'PlayingPiano',   'PlayingTabla']\n",
    "\n",
    "for tvn in examples:\n",
    "    interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = 0 if tvn in music else 1\n",
    "    y = 1-np.array([pred[idx] for pred in y_pred_test_prob[tvn]])  # np.arange(len(test_data.class_names) + 1)    \n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "    plt.savefig(\"./hw6_results/fig_one_label/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "music = ['PlayingCello',  'PlayingDhol',   'PlayingGuitar',  'PlayingSitar',  'PlayingViolin', \n",
    "         'PlayingDaf',    'PlayingFlute',  'PlayingPiano',   'PlayingTabla']\n",
    "\n",
    "\n",
    "\n",
    "timeTrueLabel = {}\n",
    "for tvn in examples:\n",
    "    interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = 0 if tvn in music else 1\n",
    "    y_one_label = 1-np.array([pred[idx] for pred in y_pred_test_prob[tvn]]) \n",
    "    x = x[:len(y_one_label)]    \n",
    "    timeTrueLabel[tvn] = {tvn[2:-8]: [[str(time), str(y_one_label[idx])] for idx, time in enumerate(x)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./hw6_results/fig_one_label/example.json', 'w') as fp:\n",
    "    json.dump(timeTrueLabel, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "tvns = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "avi_path = \"/data/qq/CSCE689/video/UCF-101\"\n",
    "clip_duration_dict = {}\n",
    "real_prediction_dict = {}\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "i = 0\n",
    "for tvn in tvns:\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    clip = VideoFileClip(os.path.join(avi_path, tvn[2:-8], tvn + \".avi\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration]\n",
    "#     real_prediction_dict[tvn] = test_results['results'][tvn][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_results/clip_duration_dict.json', 'w') as fp:\n",
    "    json.dump(clip_duration_dict, fp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "clf = load('./hw6_results/logistic_ucf.joblib') \n",
    "\n",
    "x_test_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_train.npy')\n",
    "x_test_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_train.npy')\n",
    "x_test = np.concatenate([x_test_1, x_test_2], axis=1)\n",
    "y_test = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "y_pred_test_raw = clf.predict(x_test)\n",
    "y_pred_test_prob_raw = clf.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# get ground-truth split\n",
    "# name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "name_to_int = {'Yes': 1, 'No': 0}\n",
    "\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test, y_pred_test_prob, y_pred_test_final = {}, {}, {}\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test[y_test[split]] = y_pred_test_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_prob[y_test[split]] = y_pred_test_prob_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_final[y_test[split]] = np.argmax(np.mean(y_pred_test_prob_raw[split:split_idx[i+1]], axis=0))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video -- HW6 ensemble ResNext-101 + ResNet 50 + logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "music = ['PlayingCello',  'PlayingDhol',   'PlayingGuitar',  'PlayingSitar',  'PlayingViolin', \n",
    "         'PlayingDaf',    'PlayingFlute',  'PlayingPiano',   'PlayingTabla']\n",
    "\n",
    "for tvn in y_pred_test_prob:\n",
    "    interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = 0 if tvn in music else 1\n",
    "    y = 1-np.array([pred[idx] for pred in y_pred_test_prob[tvn]])  # np.arange(len(test_data.class_names) + 1)    \n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "    plt.savefig(\"./hw6_results/fig_one_label/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json file for each video -- HW6 ensemble ResNext-101 + ResNet 50 + logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "timeTrueLabel = {}\n",
    "for tvn in y_pred_test_prob:\n",
    "    interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = 0 if tvn in music else 1\n",
    "    y_one_label = 1-np.array([pred[idx] for pred in y_pred_test_prob[tvn]]) \n",
    "    x = x[:len(y_one_label)]    \n",
    "    timeTrueLabel[tvn] = {tvn[2:-8]: [[str(time), str(y_one_label[idx])] for idx, time in enumerate(x)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_results/fig_one_label/timeLabel.json', 'w') as fp:\n",
    "    json.dump(timeTrueLabel, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
