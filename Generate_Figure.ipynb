{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'results2',\n",
    "    \"dataset\": 'ucf101-music', # 'ucf101',\n",
    "    \"n_classes\": 9, \n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 16,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'activitynet',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 2,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"ft_begin_index\": 5,\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 2,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"pretained_model_name\": 'resnext-101-kinetics',\n",
    "    \"model_depth\": 101,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1,\n",
    "    'test_subset': 'test',\n",
    "})\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path = '/data/qq/CSCE689/video/'\n",
    "opt.video_path = opt.root_path + 'UCF-music/'\n",
    "opt.annotation_path = opt.root_path+'UCF-music-annotation/ucf101_music_with_testing.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnext\n",
    "\n",
    "# construct model architecture\n",
    "model = resnext.resnet101(\n",
    "            num_classes=opt.n_classes,\n",
    "            shortcut_type=opt.resnet_shortcut,\n",
    "            cardinality=opt.resnext_cardinality,\n",
    "            sample_size=opt.sample_size,\n",
    "            sample_duration=opt.sample_duration)\n",
    "\n",
    "model = model.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "# load best weight (we can also refit the model on the combined train-val dataset, \n",
    "# but here we simple load the weight and do the final testing)\n",
    "pretrain = torch.load('./results1/save_50.pth')\n",
    "model.load_state_dict(pretrain['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/149]\n"
     ]
    }
   ],
   "source": [
    "from datasets.ucf101 import UCF101\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = VideoID() # ClassLabel()\n",
    "\n",
    "\n",
    "\n",
    "# get test data\n",
    "test_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/testing.py:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(inputs, volatile=True)\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/testing.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs = F.softmax(outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/162]\tTime 0.986 (0.986)\tData 0.846 (0.846)\t\n",
      "[2/162]\tTime 0.138 (0.562)\tData 0.011 (0.428)\t\n",
      "[3/162]\tTime 0.124 (0.416)\tData 0.007 (0.288)\t\n",
      "[4/162]\tTime 0.135 (0.346)\tData 0.008 (0.218)\t\n",
      "[5/162]\tTime 0.280 (0.333)\tData 0.156 (0.206)\t\n",
      "[6/162]\tTime 0.124 (0.298)\tData 0.006 (0.172)\t\n",
      "[7/162]\tTime 0.124 (0.273)\tData 0.005 (0.148)\t\n",
      "[8/162]\tTime 0.127 (0.255)\tData 0.007 (0.131)\t\n",
      "[9/162]\tTime 0.243 (0.253)\tData 0.123 (0.130)\t\n",
      "[10/162]\tTime 0.123 (0.240)\tData 0.008 (0.118)\t\n",
      "[11/162]\tTime 0.124 (0.230)\tData 0.006 (0.108)\t\n",
      "[12/162]\tTime 0.128 (0.221)\tData 0.007 (0.099)\t\n",
      "[13/162]\tTime 0.328 (0.230)\tData 0.202 (0.107)\t\n",
      "[14/162]\tTime 0.127 (0.222)\tData 0.007 (0.100)\t\n",
      "[15/162]\tTime 0.126 (0.216)\tData 0.007 (0.094)\t\n",
      "[16/162]\tTime 0.131 (0.211)\tData 0.008 (0.088)\t\n",
      "[17/162]\tTime 0.324 (0.217)\tData 0.203 (0.095)\t\n",
      "[18/162]\tTime 0.128 (0.212)\tData 0.005 (0.090)\t\n",
      "[19/162]\tTime 0.130 (0.208)\tData 0.007 (0.086)\t\n",
      "[20/162]\tTime 0.124 (0.204)\tData 0.007 (0.082)\t\n",
      "[21/162]\tTime 0.253 (0.206)\tData 0.131 (0.084)\t\n",
      "[22/162]\tTime 0.128 (0.203)\tData 0.006 (0.081)\t\n",
      "[23/162]\tTime 0.127 (0.199)\tData 0.010 (0.078)\t\n",
      "[24/162]\tTime 0.123 (0.196)\tData 0.007 (0.075)\t\n",
      "[25/162]\tTime 0.276 (0.199)\tData 0.151 (0.078)\t\n",
      "[26/162]\tTime 0.127 (0.197)\tData 0.006 (0.075)\t\n",
      "[27/162]\tTime 0.128 (0.194)\tData 0.009 (0.072)\t\n",
      "[28/162]\tTime 0.132 (0.192)\tData 0.008 (0.070)\t\n",
      "[29/162]\tTime 0.279 (0.195)\tData 0.149 (0.073)\t\n",
      "[30/162]\tTime 0.128 (0.193)\tData 0.007 (0.071)\t\n",
      "[31/162]\tTime 0.127 (0.190)\tData 0.008 (0.069)\t\n",
      "[32/162]\tTime 0.134 (0.189)\tData 0.012 (0.067)\t\n",
      "[33/162]\tTime 0.301 (0.192)\tData 0.181 (0.070)\t\n",
      "[34/162]\tTime 0.125 (0.190)\tData 0.007 (0.069)\t\n",
      "[35/162]\tTime 0.126 (0.188)\tData 0.007 (0.067)\t\n",
      "[36/162]\tTime 0.130 (0.187)\tData 0.008 (0.065)\t\n",
      "[37/162]\tTime 0.280 (0.189)\tData 0.157 (0.068)\t\n",
      "[38/162]\tTime 0.129 (0.188)\tData 0.007 (0.066)\t\n",
      "[39/162]\tTime 0.126 (0.186)\tData 0.007 (0.064)\t\n",
      "[40/162]\tTime 0.134 (0.185)\tData 0.010 (0.063)\t\n",
      "[41/162]\tTime 0.223 (0.186)\tData 0.100 (0.064)\t\n",
      "[42/162]\tTime 0.125 (0.184)\tData 0.007 (0.063)\t\n",
      "[43/162]\tTime 0.127 (0.183)\tData 0.008 (0.061)\t\n",
      "[44/162]\tTime 0.133 (0.182)\tData 0.008 (0.060)\t\n",
      "[45/162]\tTime 0.228 (0.183)\tData 0.105 (0.061)\t\n",
      "[46/162]\tTime 0.122 (0.181)\tData 0.007 (0.060)\t\n",
      "[47/162]\tTime 0.126 (0.180)\tData 0.006 (0.059)\t\n",
      "[48/162]\tTime 0.130 (0.179)\tData 0.008 (0.058)\t\n",
      "[49/162]\tTime 0.262 (0.181)\tData 0.141 (0.059)\t\n",
      "[50/162]\tTime 0.126 (0.180)\tData 0.007 (0.058)\t\n",
      "[51/162]\tTime 0.128 (0.179)\tData 0.008 (0.057)\t\n",
      "[52/162]\tTime 0.130 (0.178)\tData 0.007 (0.056)\t\n",
      "[53/162]\tTime 0.265 (0.179)\tData 0.145 (0.058)\t\n",
      "[54/162]\tTime 0.126 (0.179)\tData 0.005 (0.057)\t\n",
      "[55/162]\tTime 0.124 (0.178)\tData 0.008 (0.056)\t\n",
      "[56/162]\tTime 0.130 (0.177)\tData 0.008 (0.055)\t\n",
      "[57/162]\tTime 0.256 (0.178)\tData 0.133 (0.057)\t\n",
      "[58/162]\tTime 0.127 (0.177)\tData 0.005 (0.056)\t\n",
      "[59/162]\tTime 0.134 (0.176)\tData 0.008 (0.055)\t\n",
      "[60/162]\tTime 0.133 (0.176)\tData 0.011 (0.054)\t\n",
      "[61/162]\tTime 0.268 (0.177)\tData 0.145 (0.056)\t\n",
      "[62/162]\tTime 0.128 (0.176)\tData 0.006 (0.055)\t\n",
      "[63/162]\tTime 0.127 (0.176)\tData 0.008 (0.054)\t\n",
      "[64/162]\tTime 0.127 (0.175)\tData 0.007 (0.054)\t\n",
      "[65/162]\tTime 0.298 (0.177)\tData 0.174 (0.055)\t\n",
      "[66/162]\tTime 0.128 (0.176)\tData 0.006 (0.055)\t\n",
      "[67/162]\tTime 0.134 (0.175)\tData 0.010 (0.054)\t\n",
      "[68/162]\tTime 0.127 (0.175)\tData 0.008 (0.053)\t\n",
      "[69/162]\tTime 0.284 (0.176)\tData 0.164 (0.055)\t\n",
      "[70/162]\tTime 0.126 (0.176)\tData 0.007 (0.054)\t\n",
      "[71/162]\tTime 0.131 (0.175)\tData 0.009 (0.054)\t\n",
      "[72/162]\tTime 0.125 (0.174)\tData 0.006 (0.053)\t\n",
      "[73/162]\tTime 0.301 (0.176)\tData 0.180 (0.055)\t\n",
      "[74/162]\tTime 0.127 (0.175)\tData 0.007 (0.054)\t\n",
      "[75/162]\tTime 0.127 (0.175)\tData 0.008 (0.053)\t\n",
      "[76/162]\tTime 0.126 (0.174)\tData 0.006 (0.053)\t\n",
      "[77/162]\tTime 0.293 (0.176)\tData 0.172 (0.054)\t\n",
      "[78/162]\tTime 0.129 (0.175)\tData 0.005 (0.054)\t\n",
      "[79/162]\tTime 0.128 (0.174)\tData 0.007 (0.053)\t\n",
      "[80/162]\tTime 0.126 (0.174)\tData 0.007 (0.053)\t\n",
      "[81/162]\tTime 0.286 (0.175)\tData 0.170 (0.054)\t\n",
      "[82/162]\tTime 0.123 (0.175)\tData 0.006 (0.053)\t\n",
      "[83/162]\tTime 0.128 (0.174)\tData 0.007 (0.053)\t\n",
      "[84/162]\tTime 0.128 (0.173)\tData 0.006 (0.052)\t\n",
      "[85/162]\tTime 0.243 (0.174)\tData 0.122 (0.053)\t\n",
      "[86/162]\tTime 0.127 (0.174)\tData 0.007 (0.053)\t\n",
      "[87/162]\tTime 0.127 (0.173)\tData 0.007 (0.052)\t\n",
      "[88/162]\tTime 0.127 (0.173)\tData 0.006 (0.052)\t\n",
      "[89/162]\tTime 0.408 (0.175)\tData 0.165 (0.053)\t\n",
      "[90/162]\tTime 0.129 (0.175)\tData 0.006 (0.052)\t\n",
      "[91/162]\tTime 0.128 (0.174)\tData 0.007 (0.052)\t\n",
      "[92/162]\tTime 0.129 (0.174)\tData 0.007 (0.051)\t\n",
      "[93/162]\tTime 0.128 (0.173)\tData 0.010 (0.051)\t\n",
      "[94/162]\tTime 0.129 (0.173)\tData 0.006 (0.050)\t\n",
      "[95/162]\tTime 0.123 (0.172)\tData 0.007 (0.050)\t\n",
      "[96/162]\tTime 0.125 (0.172)\tData 0.007 (0.049)\t\n",
      "[97/162]\tTime 0.198 (0.172)\tData 0.081 (0.050)\t\n",
      "[98/162]\tTime 0.124 (0.172)\tData 0.006 (0.049)\t\n",
      "[99/162]\tTime 0.127 (0.171)\tData 0.008 (0.049)\t\n",
      "[100/162]\tTime 0.131 (0.171)\tData 0.007 (0.049)\t\n",
      "[101/162]\tTime 0.251 (0.172)\tData 0.124 (0.049)\t\n",
      "[102/162]\tTime 0.125 (0.171)\tData 0.005 (0.049)\t\n",
      "[103/162]\tTime 0.127 (0.171)\tData 0.008 (0.048)\t\n",
      "[104/162]\tTime 0.125 (0.170)\tData 0.006 (0.048)\t\n",
      "[105/162]\tTime 0.239 (0.171)\tData 0.116 (0.049)\t\n",
      "[106/162]\tTime 0.128 (0.170)\tData 0.007 (0.048)\t\n",
      "[107/162]\tTime 0.129 (0.170)\tData 0.008 (0.048)\t\n",
      "[108/162]\tTime 0.131 (0.170)\tData 0.006 (0.048)\t\n",
      "[109/162]\tTime 0.228 (0.170)\tData 0.109 (0.048)\t\n",
      "[110/162]\tTime 0.127 (0.170)\tData 0.007 (0.048)\t\n",
      "[111/162]\tTime 0.136 (0.170)\tData 0.011 (0.047)\t\n",
      "[112/162]\tTime 0.129 (0.169)\tData 0.007 (0.047)\t\n",
      "[113/162]\tTime 0.236 (0.170)\tData 0.115 (0.048)\t\n",
      "[114/162]\tTime 0.126 (0.169)\tData 0.006 (0.047)\t\n",
      "[115/162]\tTime 0.163 (0.169)\tData 0.042 (0.047)\t\n",
      "[116/162]\tTime 0.128 (0.169)\tData 0.005 (0.047)\t\n",
      "[117/162]\tTime 0.204 (0.169)\tData 0.079 (0.047)\t\n",
      "[118/162]\tTime 0.126 (0.169)\tData 0.006 (0.047)\t\n",
      "[119/162]\tTime 0.202 (0.169)\tData 0.081 (0.047)\t\n",
      "[120/162]\tTime 0.131 (0.169)\tData 0.006 (0.047)\t\n",
      "[121/162]\tTime 0.173 (0.169)\tData 0.047 (0.047)\t\n",
      "[122/162]\tTime 0.127 (0.169)\tData 0.006 (0.046)\t\n",
      "[123/162]\tTime 0.223 (0.169)\tData 0.102 (0.047)\t\n",
      "[124/162]\tTime 0.129 (0.169)\tData 0.006 (0.047)\t\n",
      "[125/162]\tTime 0.148 (0.169)\tData 0.023 (0.046)\t\n",
      "[126/162]\tTime 0.128 (0.168)\tData 0.006 (0.046)\t\n",
      "[127/162]\tTime 0.272 (0.169)\tData 0.145 (0.047)\t\n",
      "[128/162]\tTime 0.126 (0.169)\tData 0.007 (0.047)\t\n",
      "[129/162]\tTime 0.129 (0.168)\tData 0.008 (0.046)\t\n",
      "[130/162]\tTime 0.129 (0.168)\tData 0.006 (0.046)\t\n",
      "[131/162]\tTime 0.257 (0.169)\tData 0.135 (0.047)\t\n",
      "[132/162]\tTime 0.131 (0.168)\tData 0.007 (0.046)\t\n",
      "[133/162]\tTime 0.133 (0.168)\tData 0.010 (0.046)\t\n",
      "[134/162]\tTime 0.128 (0.168)\tData 0.006 (0.046)\t\n",
      "[135/162]\tTime 0.214 (0.168)\tData 0.091 (0.046)\t\n",
      "[136/162]\tTime 0.127 (0.168)\tData 0.006 (0.046)\t\n",
      "[137/162]\tTime 0.182 (0.168)\tData 0.058 (0.046)\t\n",
      "[138/162]\tTime 0.128 (0.168)\tData 0.008 (0.046)\t\n",
      "[139/162]\tTime 0.165 (0.168)\tData 0.044 (0.046)\t\n",
      "[140/162]\tTime 0.128 (0.167)\tData 0.007 (0.045)\t\n",
      "[141/162]\tTime 0.216 (0.168)\tData 0.091 (0.046)\t\n",
      "[142/162]\tTime 0.128 (0.167)\tData 0.006 (0.045)\t\n",
      "[143/162]\tTime 0.148 (0.167)\tData 0.021 (0.045)\t\n",
      "[144/162]\tTime 0.126 (0.167)\tData 0.006 (0.045)\t\n",
      "[145/162]\tTime 0.218 (0.167)\tData 0.095 (0.045)\t\n",
      "[146/162]\tTime 0.127 (0.167)\tData 0.006 (0.045)\t\n",
      "[147/162]\tTime 0.128 (0.167)\tData 0.009 (0.045)\t\n",
      "[148/162]\tTime 0.129 (0.167)\tData 0.006 (0.044)\t\n",
      "[149/162]\tTime 0.256 (0.167)\tData 0.131 (0.045)\t\n",
      "[150/162]\tTime 0.127 (0.167)\tData 0.006 (0.045)\t\n",
      "[151/162]\tTime 0.143 (0.167)\tData 0.021 (0.045)\t\n",
      "[152/162]\tTime 0.127 (0.167)\tData 0.006 (0.044)\t\n",
      "[153/162]\tTime 0.246 (0.167)\tData 0.123 (0.045)\t\n",
      "[154/162]\tTime 0.126 (0.167)\tData 0.006 (0.045)\t\n",
      "[155/162]\tTime 0.131 (0.167)\tData 0.009 (0.044)\t\n",
      "[156/162]\tTime 0.128 (0.166)\tData 0.006 (0.044)\t\n",
      "[157/162]\tTime 0.288 (0.167)\tData 0.159 (0.045)\t\n",
      "[158/162]\tTime 0.131 (0.167)\tData 0.007 (0.045)\t\n",
      "[159/162]\tTime 0.138 (0.167)\tData 0.012 (0.044)\t\n",
      "[160/162]\tTime 0.131 (0.166)\tData 0.007 (0.044)\t\n",
      "[161/162]\tTime 0.219 (0.167)\tData 0.093 (0.045)\t\n",
      "[162/162]\tTime 0.097 (0.166)\tData 0.007 (0.044)\t\n"
     ]
    }
   ],
   "source": [
    "from testing import final_test\n",
    "test_results, all_output_buffer = final_test(test_loader, model, opt, test_data.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['v_PlayingGuitar_g05_c03', \n",
    "            'v_PlayingViolin_g03_c03', \n",
    "            'v_PlayingCello_g07_c05', \n",
    "            'v_PlayingFlute_g07_c04',\n",
    "            'v_PlayingPiano_g01_c02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avi_path = \"/data/qq/CSCE689/video/UCF-101\"\n",
    "clip_duration_dict = {}\n",
    "real_prediction_dict = {}\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "for tvn in test_results['results']:\n",
    "    clip = VideoFileClip(os.path.join(avi_path, tvn[2:-8], tvn + \".avi\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration, all_output_buffer[tvn]]\n",
    "    real_prediction_dict[tvn] = test_results['results'][tvn][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video  (all label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    y = np.array([np.argmax(pred) for pred in clip_duration_dict[tvn][1]]) + 1  # np.arange(len(test_data.class_names) + 1)\n",
    "    x = x[:len(y)]\n",
    "    my_yticks = [''] + list(test_data.class_names.values()) + ['']\n",
    "    plt.plot(x, y)\n",
    "    plt.yticks(np.arange(len(my_yticks) + 1), my_yticks)\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict:  \" + real_prediction_dict[tvn]['label'])\n",
    "#     plt.savefig(\"./figs/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['results']['v_PlayingTabla_g04_c04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video (one label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y = np.array([pred[idx] for pred in clip_duration_dict[tvn][1]])  # np.arange(len(test_data.class_names) + 1)    \n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "#     plt.savefig(\"./figs2/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json file for each video (one label & all label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "timeTrueLabel = {}\n",
    "timeAllLabel = {}\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y1 = np.array([pred[idx] for pred in clip_duration_dict[tvn][1]])  \n",
    "    y2 = np.array([np.argmax(pred) for pred in clip_duration_dict[tvn][1]])\n",
    "    x = x[:len(y1)]\n",
    "    \n",
    "    timeTrueLabel[tvn] = {tvn[2:-8]: [[str(time), str(y1[idx])] for idx, time in enumerate(x)]}\n",
    "    timeAllLabel[tvn] = {tvn[2:-8]: [[str(time), test_data.class_names[y2[idx]]] for idx, time in enumerate(x)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./figs2/timeTrueLabel.json', 'w') as fp:\n",
    "    json.dump(timeTrueLabel, fp)\n",
    "\n",
    "with open('./figs/timeAllLabel.json', 'w') as fp:\n",
    "    json.dump(timeAllLabel, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 5 example json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./figs2/timeTrueLabel.json', 'r') as fp:\n",
    "    j1 = json.load(fp)\n",
    "\n",
    "with open('./figs/timeAllLabel.json', 'r') as fp:\n",
    "    j2 = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(j1), len(j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig1 = {}\n",
    "# fig2 = {}\n",
    "# for e in examples:\n",
    "#     fig1[e] = j1[e]\n",
    "#     fig2[e] = j2[e]\n",
    "    \n",
    "# with open('./figs2/fig2.json', 'w') as fp:\n",
    "#     json.dump(fig1, fp)\n",
    "\n",
    "# with open('./figs/fig1.json', 'w') as fp:\n",
    "#     json.dump(fig2, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the training and validation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results1/train.log', 'r') as file:\n",
    "    train_log = file.read()\n",
    "with open('./results1/val.log', 'r') as file:\n",
    "    val_log = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = train_log.split('\\n')\n",
    "val_lines = val_log.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "for train_line in train_lines[1:-1]:\n",
    "    tmp = train_line.replace('(', ', ')\n",
    "    tmp = tmp.split(', ')\n",
    "    train_loss.append(float(tmp[1]))\n",
    "    train_acc.append(float(tmp[3]))\n",
    "    \n",
    "val_loss = []\n",
    "val_acc = []\n",
    "for val_line in val_lines[1:-1]:\n",
    "    tmp = val_line.replace('(', ', ')\n",
    "    tmp = tmp.split(', ')\n",
    "    val_loss.append(float(tmp[1]))\n",
    "    val_acc.append(float(tmp[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(1, 101)), train_loss, label = \"train loss\")\n",
    "plt.plot(list(range(1, 101)), val_loss, label = \"val loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# Set a title of the current axes.\n",
    "plt.title('Fine-tuning Loss Changing Curves')\n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(1, 101)), train_acc, label = \"train acc\")\n",
    "plt.plot(list(range(1, 101)), val_acc, label = \"val acc\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# Set a title of the current axes.\n",
    "plt.title('Fine-tuning Accuracy Changing Curves')\n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./results1/test.json', 'r') as fp:\n",
    "    test_results = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_results['results'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "name_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "predict_label = []\n",
    "predict_score = []\n",
    "for k, v in test_results['results'].items():\n",
    "    ground_truth.append(name_to_int[k[2:-8]])\n",
    "    predict_label.append(name_to_int[v[0]['label']])\n",
    "    predict_score.append(v[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(ground_truth, predict_label, target_names=list(test_data.class_names.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ground_truth, predict_label, labels=list(test_data.class_names.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "fig=plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(test_data.class_names.values()), rotation=30); \n",
    "ax.yaxis.set_ticklabels(list(test_data.class_names.values()), rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
