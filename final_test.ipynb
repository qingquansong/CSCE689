{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = local_path[0] + '/video/'\n",
    "video_path_jpg = local_path[0] + '/video_jpg/'\n",
    "pretrain_file_path = local_path[0] + '/pretrain/'\n",
    "extracted_feature_path = local_path[0] + '/extracted_features/'\n",
    "final_results_path = local_path[0] + '/final_test_results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(video_path_jpg):\n",
    "    os.makedirs(video_path_jpg)\n",
    "\n",
    "if not os.path.exists(extracted_feature_path):\n",
    "    os.makedirs(extracted_feature_path)\n",
    "\n",
    "if not os.path.exists(final_results_path):\n",
    "    os.makedirs(final_results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/video_jpg.py $video_path $video_path_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test 4213\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test1 4213\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test 4213\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test1 4213\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test 4213\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/video_jpg/8test1 4213\n"
     ]
    }
   ],
   "source": [
    "!python utils/n_frames.py $video_path_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/qq/CSCE689/3D-ResNets-PyTorch']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'results2',\n",
    "    \"dataset\": 'ucf101-music', # 'ucf101',\n",
    "    \"n_classes\": 2, \n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 64,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'kinetics',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 2,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"ft_begin_index\": 5,\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 2,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"pretained_model_name\": 'resnext-101-kinetics',\n",
    "    \"model_depth\": 101,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1,\n",
    "    'test_subset': 'test',\n",
    "})\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path =  local_path[0]\n",
    "opt.video_path = video_path_jpg\n",
    "# opt.annotation_path = opt.root_path + 'video/UCF-music-annotation/ucf_binary_music_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/2]\n"
     ]
    }
   ],
   "source": [
    "from datasets.no_label_binary import NoLabelBinary\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = VideoID() # ClassLabel()\n",
    "\n",
    "\n",
    "\n",
    "# get test data\n",
    "test_data = NoLabelBinary(\n",
    "    opt.video_path,\n",
    "    None,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=64)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D ResNeXt-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnext\n",
    "\n",
    "# construct model architecture\n",
    "model_rxt101 = resnext.resnet101(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                cardinality=opt.resnext_cardinality,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "\n",
    "model_rxt101 = model_rxt101.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model_rxt101 = nn.DataParallel(model_rxt101, device_ids=None)\n",
    "\n",
    "\n",
    "### Load pretrained weight\n",
    "# customize the pretrained model path\n",
    "pretrain = torch.load(pretrain_file_path + 'resnext-101-kinetics.pth')\n",
    "pretrain_dict = pretrain['state_dict']\n",
    "\n",
    "# do not load the last layer since we want to fine-tune it\n",
    "pretrain_dict.pop('module.fc.weight')\n",
    "pretrain_dict.pop('module.fc.bias')\n",
    "model_dict = model_rxt101.state_dict()\n",
    "model_dict.update(pretrain_dict) \n",
    "model_rxt101.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract test set features:\n",
      "0\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(16, 2048)\n",
      "(4, 2048)\n"
     ]
    }
   ],
   "source": [
    "# register layer index to extract the features by forwarding all the video clips\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model_rxt101.module.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "model_rxt101.eval()\n",
    "\n",
    "\n",
    "# forward all the videos to extract features\n",
    "avgpool_test = []\n",
    "targets_test = []\n",
    "with torch.no_grad():\n",
    "    print(\"Extract test set features:\")\n",
    "    for i, (inputs, target) in enumerate(test_loader):\n",
    "        if i % 30 == 0:\n",
    "            print(i)\n",
    "        output = model_rxt101(inputs)\n",
    "        avgpool_test.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "        targets_test.append(target)\n",
    "#         print(avgpool_test[-1].numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_test_np = np.concatenate([i.numpy() for i in avgpool_test], axis=0)\n",
    "np.save(extracted_feature_path + 'resnext101_avgpool_test.npy', avgpool_test_np)\n",
    "\n",
    "targets_test_np = np.concatenate(np.array(targets_test), axis=0)\n",
    "np.save(extracted_feature_path + 'class_names_test.npy', targets_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnet.py:145: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnet\n",
    "\n",
    "# construct model architecture\n",
    "model_rt50 = resnet.resnet50(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "\n",
    "model_rt50 = model_rt50.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model_rt50 = nn.DataParallel(model_rt50, device_ids=None)\n",
    "\n",
    "\n",
    "### Load pretrained weight\n",
    "# customize the pretrained model path\n",
    "pretrain = torch.load(pretrain_file_path + 'resnet-50-kinetics.pth')\n",
    "pretrain_dict = pretrain['state_dict']\n",
    "\n",
    "# do not load the last layer since we want to fine-tune it\n",
    "pretrain_dict.pop('module.fc.weight')\n",
    "pretrain_dict.pop('module.fc.bias')\n",
    "model_dict = model_rt50.state_dict()\n",
    "model_dict.update(pretrain_dict) \n",
    "model_rt50.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract test set features:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# register layer index to extract the features by forwarding all the video clips\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model_rt50.module.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "model_rt50.eval()\n",
    "\n",
    "\n",
    "# forward all the videos to extract features\n",
    "avgpool_test = []\n",
    "with torch.no_grad():\n",
    "    print(\"Extract test set features:\")\n",
    "    for i, (inputs, target) in enumerate(test_loader):\n",
    "        if i % 30 == 0:\n",
    "            print(i)\n",
    "        output = model_rt50(inputs)\n",
    "        avgpool_test.append(activation['avgpool'].view(len(target), -1).cpu())\n",
    "        \n",
    "    # save the features\n",
    "    avgpool_test_np = np.concatenate([i.numpy() for i in avgpool_test], axis=0)\n",
    "    np.save(extracted_feature_path + 'resnet50_avgpool_test.npy', avgpool_test_np)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & fuse the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = np.load(extracted_feature_path + 'resnext101_avgpool_test.npy')\n",
    "x_test_2 = np.load(extracted_feature_path + 'resnet50_avgpool_test.npy')\n",
    "x_test = np.concatenate([x_test_1, x_test_2], axis=1)\n",
    "\n",
    "y_test = np.load(extracted_feature_path + 'class_names_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classification head and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hw4 best model\n",
    "# clf = load('./hw6_results/logistic2_ucf.joblib') \n",
    "# y_pred_test_raw = clf.predict(x_test_2)\n",
    "# y_pred_test_prob_raw = clf.predict_proba(x_test_2)\n",
    "\n",
    "\n",
    "# # hw5 best model\n",
    "# clf = load('./hw6_results/logistic_ucf.joblib') \n",
    "# y_pred_test_raw = clf.predict(x_test)\n",
    "# y_pred_test_prob_raw = clf.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# # hw6 best model\n",
    "# clf = load('./hw6_results/logistic1_ucf.joblib') \n",
    "# y_pred_test_raw = clf.predict(x_test_1)\n",
    "# y_pred_test_prob_raw = clf.predict_proba(x_test_1)\n",
    "\n",
    "\n",
    "# # hw8 best model\n",
    "# clf = load('./hw8_results/logistic_ucf.joblib') \n",
    "# y_pred_test_raw = clf.predict(x_test)\n",
    "# y_pred_test_prob_raw = clf.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# Final best model\n",
    "clf = load('./hw8_results/logistic1_ucf.joblib') \n",
    "y_pred_test_raw = clf.predict(x_test_1)\n",
    "y_pred_test_prob_raw = clf.predict_proba(x_test_1)\n",
    "\n",
    "\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "split_idx.append(len(y_test))\n",
    "        \n",
    "y_pred_test, y_pred_test_prob, y_pred_test_final = {}, {}, {}\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test[y_test[split]] = y_pred_test_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_prob[y_test[split]] = y_pred_test_prob_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_final[y_test[split]] = np.argmax(np.mean(y_pred_test_prob_raw[split:split_idx[i+1]], axis=0))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the length (in seconds) of each video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvns = list(y_pred_test_final.keys())\n",
    "mp4_path = video_path\n",
    "clip_duration_dict = {}\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "i = 0\n",
    "for tvn in tvns:\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    clip = VideoFileClip(os.path.join(mp4_path, tvn + \".mp4\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for tvn in clip_duration_dict:\n",
    "    interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    y_idx = np.argmax(y_pred_test_prob[tvn], 1)\n",
    "    y = y_pred_test_prob[tvn][:, 1]\n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "    plt.savefig(final_results_path + '625007598_' + tvn, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "timeTrueLabel = {}\n",
    "for tvn in clip_duration_dict:\n",
    "    if tvn in y_pred_test_prob:\n",
    "        interval = clip_duration_dict[tvn][0]/list(y_test).count(tvn)\n",
    "        x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "        y_idx = np.argmax(y_pred_test_prob[tvn], 1)\n",
    "        y = y_pred_test_prob[tvn][:, 1]\n",
    "        x = x[:len(y)]  \n",
    "        timeTrueLabel[tvn] = [[str(time), str(y[idx])] for idx, time in enumerate(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(final_results_path + '625007598_timeLabel.json', 'w') as fp:\n",
    "    json.dump(timeTrueLabel, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(final_results_path + '625007598_timeLabel.json', 'r') as fp:\n",
    "#     qq = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
