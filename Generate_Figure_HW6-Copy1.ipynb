{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'results2',\n",
    "    \"dataset\": 'ucf101-music', # 'ucf101',\n",
    "    \"n_classes\": 9, \n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 16,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'activitynet',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 2,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"ft_begin_index\": 5,\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 2,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"pretained_model_name\": 'resnext-101-kinetics',\n",
    "    \"model_depth\": 101,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1,\n",
    "    'test_subset': 'test',\n",
    "})\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path = '/data/qq/CSCE689/'\n",
    "opt.video_path = opt.root_path + 'video/UCF-101-jpg/'\n",
    "opt.annotation_path = opt.root_path + 'video/UCF-music-annotation/ucf_binary_music_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnext\n",
    "\n",
    "# construct model architecture\n",
    "model = resnext.resnet101(\n",
    "            num_classes=opt.n_classes,\n",
    "            shortcut_type=opt.resnet_shortcut,\n",
    "            cardinality=opt.resnext_cardinality,\n",
    "            sample_size=opt.sample_size,\n",
    "            sample_duration=opt.sample_duration)\n",
    "\n",
    "model = model.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "# load best weight (we can also refit the model on the combined train-val dataset, \n",
    "# but here we simple load the weight and do the final testing)\n",
    "pretrain = torch.load('./results1/save_50.pth')\n",
    "model.load_state_dict(pretrain['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/1600]\n",
      "dataset loading [1000/1600]\n"
     ]
    }
   ],
   "source": [
    "from datasets.ucf101 import UCF101\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = VideoID() # ClassLabel()\n",
    "\n",
    "\n",
    "\n",
    "# get test data\n",
    "test_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['v_PlayingGuitar_g05_c03', \n",
    "            'v_PlayingViolin_g03_c03', \n",
    "            'v_PlayingCello_g07_c05', \n",
    "            'v_PlayingFlute_g07_c04',\n",
    "            'v_PlayingPiano_g01_c02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18873"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tvns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b8f935c9fb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtvn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtvns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavi_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".avi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mclip_duration_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtvn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     real_prediction_dict[tvn] = test_results['results'][tvn][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/csce689/lib/python3.6/site-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/csce689/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 33\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/csce689/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mpopen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"creationflags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x08000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    705\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1288\u001b[0m                 \u001b[0merrpipe_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m                     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m                     \u001b[0merrpipe_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tvns = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "avi_path = \"/data/qq/CSCE689/video/UCF-101\"\n",
    "clip_duration_dict = {}\n",
    "real_prediction_dict = {}\n",
    "import os\n",
    "import time\n",
    "from moviepy.editor import VideoFileClip\n",
    "i= 0\n",
    "a1 = time.time()\n",
    "for tvn in tvns:\n",
    "    clip = VideoFileClip(os.path.join(avi_path, tvn[2:-8], tvn + \".avi\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration]\n",
    "#     real_prediction_dict[tvn] = test_results['results'][tvn][0]\n",
    "    i+=1\n",
    "    if i % 50==0:\n",
    "        print(i)\n",
    "        a2 = time.time()\n",
    "        print(a2-a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "clf = load('./hw6_results/logistic_ucf.joblib') \n",
    "\n",
    "x_test_1 = np.load(opt.root_path + 'feature_ucf_all/resnext101_avgpool_train.npy')\n",
    "x_test_2 = np.load(opt.root_path + 'feature_ucf_all/resnet50_avgpool_train.npy')\n",
    "x_test = np.concatenate([x_test_1, x_test_2], axis=1)\n",
    "y_test = np.load(opt.root_path + 'feature_ucf_all/class_names_ucf_test.npy')\n",
    "y_pred_test_raw = clf.predict(x_test)\n",
    "y_pred_test_prob_raw = clf.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# get ground-truth split\n",
    "# name_to_int = {v:k for k,v in test_data.class_names.items()}\n",
    "name_to_int = {'Yes': 1, 'No': 0}\n",
    "\n",
    "\n",
    "split_idx = []\n",
    "for idx, y_name in enumerate(y_test):\n",
    "    if idx == 0 or y_name != y_test[idx-1]:\n",
    "        split_idx.append(idx)\n",
    "\n",
    "y_pred_test, y_pred_test_prob, y_pred_test_final = {}, {}, {}\n",
    "for i, split in enumerate(split_idx):\n",
    "    if i < len(split_idx) - 1:\n",
    "        y_pred_test[y_test[split]] = y_pred_test_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_prob[y_test[split]] = y_pred_test_prob_raw[split:split_idx[i+1]]\n",
    "        y_pred_test_final[y_test[split]] = np.argmax(np.mean(y_pred_test_prob_raw[split:split_idx[i+1]], axis=0))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video  (all label) -- HW6 ensemble ResNext-101 + ResNet 50 +  logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# for tvn in tvns:\n",
    "#     interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "#     x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "#     y = y_pred_test[tvn] + 1  # np.arange(len(test_data.class_names) + 1)\n",
    "#     x = x[:len(y)]\n",
    "#     my_yticks = [''] + list(test_data.class_names.values()) + ['']\n",
    "#     plt.plot(x, y)\n",
    "#     plt.yticks(np.arange(len(my_yticks) + 1), my_yticks)\n",
    "#     plt.xlabel ('time/sec')\n",
    "#     plt.ylabel ('label')\n",
    "#     plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict:  \" + test_data.class_names[y_pred_test_final[tvn]])\n",
    "#     plt.savefig(\"./hw6_results/fig_all_label/\" + tvn, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video (one label) -- HW6 ensemble ResNext-101 + ResNet 50 + logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "for tvn in tvns:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y = np.array([pred[idx] for pred in y_pred_test_prob[tvn]])  # np.arange(len(test_data.class_names) + 1)    \n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "    plt.savefig(\"./hw6_results/fig_one_label/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json file for each video (one label & all label) -- HW6 ensemble ResNext-101 + ResNet 50 + logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "timeTrueLabel = {}\n",
    "timeAllLabel = {}\n",
    "for tvn in tvns:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y_one_label = np.array([pred[idx] for pred in y_pred_test_prob[tvn]]) \n",
    "    y_all_label = y_pred_test[tvn]\n",
    "    x = x[:len(y_one_label)]\n",
    "    \n",
    "    timeTrueLabel[tvn] = {tvn[2:-8]: [[str(time), str(y_one_label[idx])] for idx, time in enumerate(x)]}\n",
    "    timeAllLabel[tvn] = {tvn[2:-8]: [[str(time), test_data.class_names[y_all_label[idx]]] for idx, time in enumerate(x)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./hw6_results/fig_one_label/timeTrueLabel.json', 'w') as fp:\n",
    "#     json.dump(timeTrueLabel, fp)\n",
    "\n",
    "# with open('./hw6_results/fig_all_label/timeAllLabel.json', 'w') as fp:\n",
    "#     json.dump(timeAllLabel, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 5 example json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./hw6_results/fig_one_label/timeTrueLabel.json', 'r') as fp:\n",
    "    j_one_label = json.load(fp)\n",
    "\n",
    "with open('./hw6_results/fig_all_label/timeAllLabel.json', 'r') as fp:\n",
    "    j_all_label = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 148)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(j_one_label), len(j_all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_one_label = {}\n",
    "fig_all_label = {}\n",
    "for e in examples:\n",
    "    fig_one_label[e] = j_one_label[e]\n",
    "    fig_all_label[e] = j_all_label[e]\n",
    "    \n",
    "# with open('./hw6_results/fig_one_label/example_one_label.json', 'w') as fp:\n",
    "#     json.dump(fig_one_label, fp)\n",
    "\n",
    "# with open('./hw6_results/fig_all_label/example_all_label.json', 'w') as fp:\n",
    "#     json.dump(fig_all_label, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
