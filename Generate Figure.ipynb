{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import test\n",
    "import collections\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "    \"result_path\": 'results2',\n",
    "    \"dataset\": 'ucf101-music', # 'ucf101',\n",
    "    \"n_classes\": 9, \n",
    "    \"sample_size\": 112,\n",
    "    \"sample_duration\": 16,\n",
    "    \"initial_scale\": 1.0,\n",
    "    \"n_scales\": 5,\n",
    "    \"scale_step\": 0.84089641525,\n",
    "    \"train_crop\": 'corner',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dampening\": 0.9,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"mean_dataset\": 'activitynet',\n",
    "    \"no_mean_norm\": False,\n",
    "    \"std_norm\": False,\n",
    "    \"nesterov\": False,\n",
    "    \"optimizer\": 'sgd',\n",
    "    \"lr_patience\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 2,\n",
    "    \"begin_epoch\": 1,\n",
    "    \"n_val_samples\": 3,\n",
    "    \"ft_begin_index\": 5,\n",
    "    \"scale_in_test\": 1.0,\n",
    "    \"crop_position_in_test\": 'c',\n",
    "    \"no_softmax_in_test\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"n_threads\": 4,\n",
    "    \"checkpoint\": 2,\n",
    "    \"no_hflip\": False,\n",
    "    \"norm_value\": 1,\n",
    "    \"model\": 'resnet',\n",
    "    \"pretained_model_name\": 'resnext-101-kinetics',\n",
    "    \"model_depth\": 101,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"wide_resnet_k\": 2,\n",
    "    \"resnext_cardinality\": 32,\n",
    "    \"manual_seed\": 1,\n",
    "    'test_subset': 'test',\n",
    "})\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "opt.root_path = '/data/qq/CSCE689/video/'\n",
    "opt.video_path = opt.root_path + 'UCF-music/'\n",
    "opt.annotation_path = opt.root_path+'UCF-music-annotation/ucf101_music_with_testing.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two gpu devices on the server, you can customize it depending on how many available gpu devices you have\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/models/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnext\n",
    "\n",
    "# construct model architecture\n",
    "model = resnext.resnet101(\n",
    "            num_classes=opt.n_classes,\n",
    "            shortcut_type=opt.resnet_shortcut,\n",
    "            cardinality=opt.resnext_cardinality,\n",
    "            sample_size=opt.sample_size,\n",
    "            sample_duration=opt.sample_duration)\n",
    "\n",
    "model = model.cuda()\n",
    "# wrap the current model again in nn.DataParallel / or we can just remove the .module keys.\n",
    "model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "# load best weight (we can also refit the model on the combined train-val dataset, \n",
    "# but here we simple load the weight and do the final testing)\n",
    "pretrain = torch.load('./results1/save_200.pth')\n",
    "model.load_state_dict(pretrain['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/149]\n"
     ]
    }
   ],
   "source": [
    "from datasets.ucf101 import UCF101\n",
    "\n",
    "mean = get_mean(opt.norm_value, dataset='kinetics')\n",
    "std = get_std(opt.norm_value)\n",
    "norm_method = Normalize(mean, [1,1,1])\n",
    "\n",
    "\n",
    "spatial_transform = Compose([\n",
    "    Scale(opt.sample_size),\n",
    "    CornerCrop(opt.sample_size, 'c'),\n",
    "    ToTensor(opt.norm_value), norm_method\n",
    "])\n",
    "\n",
    "temporal_transform = LoopPadding(opt.sample_duration)\n",
    "target_transform = VideoID() # ClassLabel()\n",
    "\n",
    "\n",
    "\n",
    "# get test data\n",
    "test_data = UCF101(\n",
    "    opt.video_path,\n",
    "    opt.annotation_path,\n",
    "    'testing',\n",
    "    0,\n",
    "    spatial_transform=spatial_transform,\n",
    "    temporal_transform=temporal_transform,\n",
    "    target_transform=target_transform,\n",
    "    sample_duration=16)\n",
    "\n",
    "\n",
    "# wrap test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_threads,\n",
    "    pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qq/CSCE689/3D-ResNets-PyTorch/testing.py:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(inputs, volatile=True)\n",
      "/home/qq/CSCE689/3D-ResNets-PyTorch/testing.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs = F.softmax(outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/162]\tTime 1.416 (1.416)\tData 1.268 (1.268)\t\n",
      "[2/162]\tTime 0.120 (0.768)\tData 0.010 (0.639)\t\n",
      "[3/162]\tTime 0.108 (0.548)\tData 0.009 (0.429)\t\n",
      "[4/162]\tTime 0.107 (0.438)\tData 0.008 (0.324)\t\n",
      "[5/162]\tTime 0.495 (0.449)\tData 0.401 (0.339)\t\n",
      "[6/162]\tTime 0.109 (0.392)\tData 0.008 (0.284)\t\n",
      "[7/162]\tTime 0.111 (0.352)\tData 0.013 (0.245)\t\n",
      "[8/162]\tTime 0.105 (0.321)\tData 0.007 (0.216)\t\n",
      "[9/162]\tTime 0.471 (0.338)\tData 0.369 (0.233)\t\n",
      "[10/162]\tTime 0.109 (0.315)\tData 0.009 (0.210)\t\n",
      "[11/162]\tTime 0.114 (0.297)\tData 0.016 (0.193)\t\n",
      "[12/162]\tTime 0.103 (0.281)\tData 0.009 (0.177)\t\n",
      "[13/162]\tTime 0.542 (0.301)\tData 0.438 (0.197)\t\n",
      "[14/162]\tTime 0.109 (0.287)\tData 0.014 (0.184)\t\n",
      "[15/162]\tTime 0.107 (0.275)\tData 0.007 (0.172)\t\n",
      "[16/162]\tTime 0.111 (0.265)\tData 0.008 (0.162)\t\n",
      "[17/162]\tTime 0.479 (0.277)\tData 0.378 (0.175)\t\n",
      "[18/162]\tTime 0.102 (0.268)\tData 0.008 (0.166)\t\n",
      "[19/162]\tTime 0.106 (0.259)\tData 0.010 (0.157)\t\n",
      "[20/162]\tTime 0.113 (0.252)\tData 0.013 (0.150)\t\n",
      "[21/162]\tTime 0.350 (0.256)\tData 0.255 (0.155)\t\n",
      "[22/162]\tTime 0.117 (0.250)\tData 0.017 (0.149)\t\n",
      "[23/162]\tTime 0.112 (0.244)\tData 0.011 (0.143)\t\n",
      "[24/162]\tTime 0.176 (0.241)\tData 0.081 (0.140)\t\n",
      "[25/162]\tTime 0.315 (0.244)\tData 0.219 (0.143)\t\n",
      "[26/162]\tTime 0.137 (0.240)\tData 0.039 (0.139)\t\n",
      "[27/162]\tTime 0.118 (0.236)\tData 0.015 (0.135)\t\n",
      "[28/162]\tTime 0.114 (0.231)\tData 0.012 (0.130)\t\n",
      "[29/162]\tTime 0.338 (0.235)\tData 0.242 (0.134)\t\n",
      "[30/162]\tTime 0.208 (0.234)\tData 0.113 (0.134)\t\n",
      "[31/162]\tTime 0.110 (0.230)\tData 0.011 (0.130)\t\n",
      "[32/162]\tTime 0.105 (0.226)\tData 0.009 (0.126)\t\n",
      "[33/162]\tTime 0.323 (0.229)\tData 0.226 (0.129)\t\n",
      "[34/162]\tTime 0.126 (0.226)\tData 0.027 (0.126)\t\n",
      "[35/162]\tTime 0.105 (0.223)\tData 0.007 (0.123)\t\n",
      "[36/162]\tTime 0.123 (0.220)\tData 0.023 (0.120)\t\n",
      "[37/162]\tTime 0.415 (0.225)\tData 0.321 (0.125)\t\n",
      "[38/162]\tTime 0.137 (0.223)\tData 0.042 (0.123)\t\n",
      "[39/162]\tTime 0.100 (0.220)\tData 0.007 (0.120)\t\n",
      "[40/162]\tTime 0.107 (0.217)\tData 0.014 (0.117)\t\n",
      "[41/162]\tTime 0.342 (0.220)\tData 0.248 (0.121)\t\n",
      "[42/162]\tTime 0.116 (0.217)\tData 0.020 (0.118)\t\n",
      "[43/162]\tTime 0.105 (0.215)\tData 0.008 (0.116)\t\n",
      "[44/162]\tTime 0.175 (0.214)\tData 0.079 (0.115)\t\n",
      "[45/162]\tTime 0.308 (0.216)\tData 0.208 (0.117)\t\n",
      "[46/162]\tTime 0.104 (0.213)\tData 0.009 (0.115)\t\n",
      "[47/162]\tTime 0.114 (0.211)\tData 0.012 (0.112)\t\n",
      "[48/162]\tTime 0.126 (0.210)\tData 0.030 (0.111)\t\n",
      "[49/162]\tTime 0.325 (0.212)\tData 0.229 (0.113)\t\n",
      "[50/162]\tTime 0.102 (0.210)\tData 0.008 (0.111)\t\n",
      "[51/162]\tTime 0.100 (0.208)\tData 0.008 (0.109)\t\n",
      "[52/162]\tTime 0.320 (0.210)\tData 0.227 (0.111)\t\n",
      "[53/162]\tTime 0.176 (0.209)\tData 0.080 (0.111)\t\n",
      "[54/162]\tTime 0.106 (0.207)\tData 0.007 (0.109)\t\n",
      "[55/162]\tTime 0.111 (0.205)\tData 0.010 (0.107)\t\n",
      "[56/162]\tTime 0.355 (0.208)\tData 0.255 (0.110)\t\n",
      "[57/162]\tTime 0.145 (0.207)\tData 0.048 (0.108)\t\n",
      "[58/162]\tTime 0.111 (0.205)\tData 0.010 (0.107)\t\n",
      "[59/162]\tTime 0.105 (0.204)\tData 0.008 (0.105)\t\n",
      "[60/162]\tTime 0.356 (0.206)\tData 0.259 (0.108)\t\n",
      "[61/162]\tTime 0.164 (0.206)\tData 0.069 (0.107)\t\n",
      "[62/162]\tTime 0.107 (0.204)\tData 0.008 (0.105)\t\n",
      "[63/162]\tTime 0.101 (0.202)\tData 0.007 (0.104)\t\n",
      "[64/162]\tTime 0.336 (0.204)\tData 0.233 (0.106)\t\n",
      "[65/162]\tTime 0.159 (0.204)\tData 0.060 (0.105)\t\n",
      "[66/162]\tTime 0.103 (0.202)\tData 0.007 (0.104)\t\n",
      "[67/162]\tTime 0.106 (0.201)\tData 0.008 (0.102)\t\n",
      "[68/162]\tTime 0.320 (0.202)\tData 0.224 (0.104)\t\n",
      "[69/162]\tTime 0.197 (0.202)\tData 0.100 (0.104)\t\n",
      "[70/162]\tTime 0.121 (0.201)\tData 0.013 (0.103)\t\n",
      "[71/162]\tTime 0.105 (0.200)\tData 0.008 (0.101)\t\n",
      "[72/162]\tTime 0.245 (0.201)\tData 0.145 (0.102)\t\n",
      "[73/162]\tTime 0.275 (0.202)\tData 0.178 (0.103)\t\n",
      "[74/162]\tTime 0.100 (0.200)\tData 0.006 (0.102)\t\n",
      "[75/162]\tTime 0.104 (0.199)\tData 0.009 (0.100)\t\n",
      "[76/162]\tTime 0.306 (0.200)\tData 0.205 (0.102)\t\n",
      "[77/162]\tTime 0.208 (0.200)\tData 0.112 (0.102)\t\n",
      "[78/162]\tTime 0.102 (0.199)\tData 0.006 (0.101)\t\n",
      "[79/162]\tTime 0.104 (0.198)\tData 0.007 (0.099)\t\n",
      "[80/162]\tTime 0.285 (0.199)\tData 0.182 (0.101)\t\n",
      "[81/162]\tTime 0.254 (0.200)\tData 0.159 (0.101)\t\n",
      "[82/162]\tTime 0.104 (0.199)\tData 0.011 (0.100)\t\n",
      "[83/162]\tTime 0.104 (0.197)\tData 0.007 (0.099)\t\n",
      "[84/162]\tTime 0.246 (0.198)\tData 0.150 (0.100)\t\n",
      "[85/162]\tTime 0.280 (0.199)\tData 0.177 (0.101)\t\n",
      "[86/162]\tTime 0.106 (0.198)\tData 0.013 (0.100)\t\n",
      "[87/162]\tTime 0.104 (0.197)\tData 0.007 (0.098)\t\n",
      "[88/162]\tTime 0.229 (0.197)\tData 0.125 (0.099)\t\n",
      "[89/162]\tTime 0.285 (0.198)\tData 0.188 (0.100)\t\n",
      "[90/162]\tTime 0.103 (0.197)\tData 0.007 (0.099)\t\n",
      "[91/162]\tTime 0.104 (0.196)\tData 0.009 (0.098)\t\n",
      "[92/162]\tTime 0.203 (0.196)\tData 0.102 (0.098)\t\n",
      "[93/162]\tTime 0.289 (0.197)\tData 0.193 (0.099)\t\n",
      "[94/162]\tTime 0.103 (0.196)\tData 0.007 (0.098)\t\n",
      "[95/162]\tTime 0.112 (0.195)\tData 0.008 (0.097)\t\n",
      "[96/162]\tTime 0.181 (0.195)\tData 0.080 (0.097)\t\n",
      "[97/162]\tTime 0.298 (0.196)\tData 0.202 (0.098)\t\n",
      "[98/162]\tTime 0.116 (0.195)\tData 0.014 (0.097)\t\n",
      "[99/162]\tTime 0.113 (0.194)\tData 0.009 (0.096)\t\n",
      "[100/162]\tTime 0.199 (0.195)\tData 0.098 (0.096)\t\n",
      "[101/162]\tTime 0.381 (0.196)\tData 0.274 (0.098)\t\n",
      "[102/162]\tTime 0.114 (0.196)\tData 0.015 (0.097)\t\n",
      "[103/162]\tTime 0.109 (0.195)\tData 0.010 (0.096)\t\n",
      "[104/162]\tTime 0.120 (0.194)\tData 0.017 (0.095)\t\n",
      "[105/162]\tTime 0.379 (0.196)\tData 0.284 (0.097)\t\n",
      "[106/162]\tTime 0.112 (0.195)\tData 0.010 (0.096)\t\n",
      "[107/162]\tTime 0.104 (0.194)\tData 0.008 (0.096)\t\n",
      "[108/162]\tTime 0.110 (0.193)\tData 0.009 (0.095)\t\n",
      "[109/162]\tTime 0.369 (0.195)\tData 0.275 (0.096)\t\n",
      "[110/162]\tTime 0.103 (0.194)\tData 0.006 (0.096)\t\n",
      "[111/162]\tTime 0.105 (0.193)\tData 0.010 (0.095)\t\n",
      "[112/162]\tTime 0.103 (0.193)\tData 0.008 (0.094)\t\n",
      "[113/162]\tTime 0.447 (0.195)\tData 0.349 (0.096)\t\n",
      "[114/162]\tTime 0.102 (0.194)\tData 0.007 (0.096)\t\n",
      "[115/162]\tTime 0.105 (0.193)\tData 0.010 (0.095)\t\n",
      "[116/162]\tTime 0.105 (0.192)\tData 0.009 (0.094)\t\n",
      "[117/162]\tTime 0.397 (0.194)\tData 0.295 (0.096)\t\n",
      "[118/162]\tTime 0.103 (0.193)\tData 0.007 (0.095)\t\n",
      "[119/162]\tTime 0.105 (0.193)\tData 0.011 (0.094)\t\n",
      "[120/162]\tTime 0.105 (0.192)\tData 0.009 (0.094)\t\n",
      "[121/162]\tTime 0.394 (0.194)\tData 0.295 (0.095)\t\n",
      "[122/162]\tTime 0.102 (0.193)\tData 0.006 (0.095)\t\n",
      "[123/162]\tTime 0.105 (0.192)\tData 0.010 (0.094)\t\n",
      "[124/162]\tTime 0.106 (0.191)\tData 0.008 (0.093)\t\n",
      "[125/162]\tTime 0.391 (0.193)\tData 0.294 (0.095)\t\n",
      "[126/162]\tTime 0.104 (0.192)\tData 0.006 (0.094)\t\n",
      "[127/162]\tTime 0.103 (0.192)\tData 0.007 (0.093)\t\n",
      "[128/162]\tTime 0.107 (0.191)\tData 0.008 (0.093)\t\n",
      "[129/162]\tTime 0.394 (0.193)\tData 0.290 (0.094)\t\n",
      "[130/162]\tTime 0.104 (0.192)\tData 0.006 (0.094)\t\n",
      "[131/162]\tTime 0.104 (0.191)\tData 0.007 (0.093)\t\n",
      "[132/162]\tTime 0.107 (0.191)\tData 0.010 (0.092)\t\n",
      "[133/162]\tTime 0.415 (0.192)\tData 0.307 (0.094)\t\n",
      "[134/162]\tTime 0.109 (0.192)\tData 0.006 (0.093)\t\n",
      "[135/162]\tTime 0.104 (0.191)\tData 0.008 (0.093)\t\n",
      "[136/162]\tTime 0.107 (0.190)\tData 0.011 (0.092)\t\n",
      "[137/162]\tTime 0.386 (0.192)\tData 0.287 (0.093)\t\n",
      "[138/162]\tTime 0.121 (0.191)\tData 0.010 (0.093)\t\n",
      "[139/162]\tTime 0.107 (0.191)\tData 0.008 (0.092)\t\n",
      "[140/162]\tTime 0.118 (0.190)\tData 0.007 (0.092)\t\n",
      "[141/162]\tTime 0.389 (0.192)\tData 0.289 (0.093)\t\n",
      "[142/162]\tTime 0.107 (0.191)\tData 0.009 (0.092)\t\n",
      "[143/162]\tTime 0.104 (0.190)\tData 0.009 (0.092)\t\n",
      "[144/162]\tTime 0.113 (0.190)\tData 0.007 (0.091)\t\n",
      "[145/162]\tTime 0.371 (0.191)\tData 0.269 (0.092)\t\n",
      "[146/162]\tTime 0.111 (0.191)\tData 0.009 (0.092)\t\n",
      "[147/162]\tTime 0.102 (0.190)\tData 0.007 (0.091)\t\n",
      "[148/162]\tTime 0.106 (0.189)\tData 0.007 (0.091)\t\n",
      "[149/162]\tTime 0.388 (0.191)\tData 0.290 (0.092)\t\n",
      "[150/162]\tTime 0.101 (0.190)\tData 0.006 (0.092)\t\n",
      "[151/162]\tTime 0.111 (0.190)\tData 0.007 (0.091)\t\n",
      "[152/162]\tTime 0.104 (0.189)\tData 0.010 (0.090)\t\n",
      "[153/162]\tTime 0.419 (0.191)\tData 0.321 (0.092)\t\n",
      "[154/162]\tTime 0.100 (0.190)\tData 0.006 (0.091)\t\n",
      "[155/162]\tTime 0.105 (0.189)\tData 0.007 (0.091)\t\n",
      "[156/162]\tTime 0.107 (0.189)\tData 0.007 (0.090)\t\n",
      "[157/162]\tTime 0.409 (0.190)\tData 0.310 (0.092)\t\n",
      "[158/162]\tTime 0.105 (0.190)\tData 0.006 (0.091)\t\n",
      "[159/162]\tTime 0.117 (0.189)\tData 0.009 (0.091)\t\n",
      "[160/162]\tTime 0.104 (0.189)\tData 0.008 (0.090)\t\n",
      "[161/162]\tTime 0.358 (0.190)\tData 0.258 (0.091)\t\n",
      "[162/162]\tTime 0.081 (0.189)\tData 0.006 (0.091)\t\n"
     ]
    }
   ],
   "source": [
    "from testing import final_test\n",
    "test_results, all_output_buffer = final_test(test_loader, model, opt, test_data.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['v_PlayingGuitar_g05_c03', \n",
    "            'v_PlayingViolin_g03_c03', \n",
    "            'v_PlayingCello_g07_c05', \n",
    "            'v_PlayingFlute_g07_c04',\n",
    "            'v_PlayingPiano_g01_c02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "avi_path = \"/data/qq/CSCE689/video/UCF-101\"\n",
    "clip_duration_dict = {}\n",
    "real_prediction_dict = {}\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "for tvn in test_results['results']:\n",
    "    clip = VideoFileClip(os.path.join(avi_path, tvn[2:-8], tvn + \".avi\"))\n",
    "    clip_duration_dict[tvn] = [clip.duration, all_output_buffer[tvn]]\n",
    "    real_prediction_dict[tvn] = test_results['results'][tvn][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video  (all label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    y = np.array([np.argmax(pred) for pred in clip_duration_dict[tvn][1]]) + 1  # np.arange(len(test_data.class_names) + 1)\n",
    "    x = x[:len(y)]\n",
    "    my_yticks = [''] + list(test_data.class_names.values()) + ['']\n",
    "    plt.plot(x, y)\n",
    "    plt.yticks(np.arange(len(my_yticks) + 1), my_yticks)\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict:  \" + real_prediction_dict[tvn]['label'])\n",
    "    plt.savefig(\"./figs/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'PlayingTabla', 'score': 0.5185579657554626},\n",
       " {'label': 'PlayingSitar', 'score': 0.14087970554828644},\n",
       " {'label': 'PlayingDhol', 'score': 0.1273767501115799},\n",
       " {'label': 'PlayingDaf', 'score': 0.11835509538650513},\n",
       " {'label': 'PlayingPiano', 'score': 0.04172874987125397},\n",
       " {'label': 'PlayingFlute', 'score': 0.025170736014842987},\n",
       " {'label': 'PlayingViolin', 'score': 0.014937996864318848},\n",
       " {'label': 'PlayingGuitar', 'score': 0.008753728121519089},\n",
       " {'label': 'PlayingCello', 'score': 0.004239281173795462}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results['results']['v_PlayingTabla_g04_c04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction plot for each video (one label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y = np.array([pred[idx] for pred in clip_duration_dict[tvn][1]])  # np.arange(len(test_data.class_names) + 1)    \n",
    "    x = x[:len(y)]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel ('time/sec')\n",
    "    plt.ylabel ('pred score for ground truth label')\n",
    "    plt.title(\"Ground Truth Label:  \" + tvn[2:-8]  + \"\\n Model Avg. Predict Score:  \" + str(np.mean(y))) # str(real_prediction_dict[tvn]['score'])\n",
    "    plt.savefig(\"./figs2/\" + tvn, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json file for each video (one label & all label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to return key for any value \n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "timeTrueLabel = {}\n",
    "timeAllLabel = {}\n",
    "for tvn in test_results['results']:\n",
    "    interval = clip_duration_dict[tvn][0]/len(clip_duration_dict[tvn][1])\n",
    "    x = np.arange(0, clip_duration_dict[tvn][0], interval) + interval\n",
    "    idx = get_key(tvn[2:-8], test_data.class_names)\n",
    "    y1 = np.array([pred[idx] for pred in clip_duration_dict[tvn][1]])  \n",
    "    y2 = np.array([np.argmax(pred) for pred in clip_duration_dict[tvn][1]])\n",
    "    x = x[:len(y1)]\n",
    "    \n",
    "    timeTrueLabel[tvn] = {tvn[2:-8]: [[str(time), str(y1[idx])] for idx, time in enumerate(x)]}\n",
    "    timeAllLabel[tvn] = {tvn[2:-8]: [[str(time), test_data.class_names[y2[idx]]] for idx, time in enumerate(x)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./figs2/timeTrueLabel.json', 'a') as fp:\n",
    "    json.dump(timeTrueLabel, fp)\n",
    "\n",
    "with open('./figs/timeAllLabel.json', 'a') as fp:\n",
    "    json.dump(timeAllLabel, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csce689",
   "language": "python",
   "name": "csce689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
